# -*- coding: utf-8 -*-
"""NLP-NN-wordembedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rYA162ntLJWxVAY6EYKfV5fvVKwip93w
"""

import tensorflow as tf
print(tf.__version__)
tf.enable_eager_execution()

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

import tensorflow_datasets as tfds
import numpy
imdb,info = tfds.load("imdb_reviews",with_info=True,as_supervised=True)
train_data = imdb['train']
test_data = imdb['test']

training_sent = []
training_labels = []

testing_sent = []
testing_labels = []

for sent,label in train_data:
  training_sent.append(str(sent.numpy()))
  training_labels.append(label.numpy())
  
for sent,label in test_data:
  testing_sent.append(str(sent.numpy()))
  testing_labels.append(label.numpy())

training_labels_final= np.array(training_labels)
testing_labels_final = np.array(testing_labels)

vocab_size = 10000
embedding_dim = 16
max_length = 120
trunc_type = 'post'
oov_tok = '<OOV>'

tokenizer = Tokenizer(num_words = vocab_size,oov_token=oov_tok)
tokenizer.fit_on_texts(training_sent)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(training_sent)
padding = pad_sequences(sequences,maxlen=max_length,padding='post',truncating=trunc_type)

test_sequences = tokenizer.texts_to_sequences(testing_sent)
test_padding = pad_sequences(test_sequences,maxlen=max_length,padding='post',truncating=trunc_type)

reverse_word_index=dict([value,key] for (key,value) in word_index.items())

def decode_review(text):
  decoded_sentence= []
  for cypher in text:
    for key in reverse_word_index.keys():
      if cypher == key:
        decoded_sentence.append(reverse_word_index[key])
  return (" ".join(decoded_sentence))

print(decode_review(padding[0]))
print(training_sent[0])

model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length = max_length),
#       tf.keras.layers.GlobalAveragePolling1D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(6,activation='relu'),
        tf.keras.layers.Dense(1,activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

num_epochs = 10
model.fit(padding,training_labels_final,epochs=num_epochs,validation_data=(test_padding,testing_labels_final))



e = model.layers[0]
weights = e.get_weights()[0]
print(weights.shape)

import io
 
out_v = io.open('vecs.tsv','w',encoding='utf-8')
out_m = io.open('meta.tsv','w',encoding = 'utf-8')
for wordnum in range(1,vocab_size):
  word = reverse_word_index[wordnum]
  embedding = weights[wordnum]
  out_m.write(word+'\n')
  out_v.write('\t'.join([str(x) for x in embedding])+"\n")
out_v.close()
out_m.close()

try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download('vecs.tsv')
  files.download('meta.tsv')

sentence = "I really think this is amazing. honest."
sequence = tokenizer.texts_to_sequences(sentence)
print(sequence)